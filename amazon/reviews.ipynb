{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk import data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = data.load('tokenizers/punkt/english.pickle')\n",
    "english_stop_words = stopwords.words('english')\n",
    "comprehend = boto3.client(\n",
    "                          service_name='comprehend', \n",
    "                          region_name='us-west-2',\n",
    "                         )\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['material quality',\n",
    "         'battery life',\n",
    "         'sound quality',\n",
    "         'volume control',\n",
    "         'tech support']\n",
    "\n",
    "reviews_raw = pd.read_csv('review_details.csv',engine='python').dropna(subset=['review_content_text'])\n",
    "reviews_deduped = reviews_raw.drop_duplicates(subset=['review_id'])\n",
    "reviews_verified = reviews_deduped[reviews_deduped.is_verified == 1]\n",
    "reviews_bluetooth = reviews_verified[(reviews_verified.review_asin == 'B074QLB1Y7') |\n",
    "                                   (reviews_verified.review_asin == 'B00P24XKS8')]\n",
    "\n",
    "reviews = reviews_verified.copy()\n",
    "\n",
    "asins = list(reviews_verified.review_asin.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    return [' '.join([lemmatizer.lemmatize(word.lower()) for word in doc.split()\n",
    "                      if word not in english_stop_words]) for doc in corpus]\n",
    "\n",
    "def sentiment(score):\n",
    "    # score [-1,1]\n",
    "    if score > 0.05:\n",
    "        return 'Positive'\n",
    "    elif score < -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Mixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(score):\n",
    "    # score [-1,1]\n",
    "    if score > 0.05:\n",
    "        return 'Positive'\n",
    "    elif score < -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Mixed'\n",
    "\n",
    "for index, row in reviews.iterrows():\n",
    "    review = row['review_content_text']\n",
    "    sentences = sentence_tokenizer.tokenize(review)\n",
    "    sentences_preprocessed = preprocess(sentences)\n",
    "    topics_sentiment = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        sentences_topic = [sentence for sentence in sentences if topic in sentence]\n",
    "        \n",
    "        if sentences_topic:\n",
    "            sentence_topic = ' '.join(sentences_topic)\n",
    "            reviews.at[index, 'topic_{}'.format(topic)] = sentence_topic\n",
    "            try:\n",
    "                vs = analyzer.polarity_scores(sentence_topic)\n",
    "                reviews.at[index, 'topic_{}_sentiment'.format(topic)] = sentiment(vs['compound'])\n",
    "            except Exception as ex:\n",
    "                print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Sentiments by Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def filter_by_sentiment(ASIN = asins,\n",
    "                        Topic=topics,\n",
    "                        Sentiment = ['Positive','Negative','Mixed']):\n",
    "    reviews_asin = reviews[reviews.review_asin == ASIN]\n",
    "    reviews_topic = reviews[reviews['topic_{}'.format(Topic)].notnull()]\n",
    "    reviews_topic['topic_{}_sentiment'.format(Topic)].value_counts().sort_index().plot(kind='bar', title=Topic)\n",
    "    return reviews_asin.loc[reviews_asin['topic_{}_sentiment'.format(Topic)] == Sentiment][\n",
    "        ['review_content_text','topic_{}'.format(Topic)]].set_index('review_content_text').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
